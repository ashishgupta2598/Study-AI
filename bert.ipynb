{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "       'Hello, how are you? I am Romeo.\\n'\n",
    "       'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "       'Nice meet you too. How are you today?\\n'\n",
    "       'Great. My baseball team won the competition.\\n'\n",
    "       'Oh Congratulations, Juliet\\n'\n",
    "       'Thanks you Romeo'\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
    "word_list = list(set(\" \".join(sentences).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'how', 'are', 'you', 'i', 'am', 'romeo'],\n",
       " ['hello', 'romeo', 'my', 'name', 'is', 'juliet', 'nice', 'to', 'meet', 'you'],\n",
       " ['nice', 'meet', 'you', 'too', 'how', 'are', 'you', 'today'],\n",
       " ['great', 'my', 'baseball', 'team', 'won', 'the', 'competition'],\n",
       " ['oh', 'congratulations', 'juliet'],\n",
       " ['thanks', 'you', 'romeo']]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = [i.split() for i in sentences]\n",
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3,0:0,1:1,2:2,3:3}\n",
    "for i,w in enumerate(word_list):\n",
    "   word_dict[w] = i + 4\n",
    "   number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "vocab_size = len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33,\n",
       " {0: '[PAD]',\n",
       "  1: '[CLS]',\n",
       "  2: '[SEP]',\n",
       "  3: '[MASK]',\n",
       "  4: 0,\n",
       "  5: 1,\n",
       "  6: 2,\n",
       "  7: 3,\n",
       "  8: 'am',\n",
       "  9: 'you',\n",
       "  10: 'the',\n",
       "  11: 'thanks',\n",
       "  12: 'meet',\n",
       "  13: 'too',\n",
       "  14: 'baseball',\n",
       "  15: 'name',\n",
       "  16: 'today',\n",
       "  17: 'are',\n",
       "  18: 'great',\n",
       "  19: 'my',\n",
       "  20: 'won',\n",
       "  21: 'juliet',\n",
       "  22: 'romeo',\n",
       "  23: 'how',\n",
       "  24: 'nice',\n",
       "  25: 'oh',\n",
       "  26: 'competition',\n",
       "  27: 'congratulations',\n",
       "  28: 'i',\n",
       "  29: 'to',\n",
       "  30: 'team',\n",
       "  31: 'is',\n",
       "  32: 'hello'})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size,number_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 20, 8, 5, 9, 19, 13, 5, 12, 2, 20, 8, 5, 9, 19, 13, 5, 12, 2])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randrange,shuffle,randint\n",
    "tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences)) \n",
    "tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']] #token embeeing kind\n",
    "input_ids =  [word_dict[i] for i in input_ids if i in word_dict] \n",
    "segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "\n",
    "segment_ids,input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'romeo', 'my', 'name', 'is', 'juliet', 'nice', 'to', 'meet', 'you']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pred = 8\n",
    "n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 5, 13, 7, 10, 12, 1, 3, 6, 11, 9, 8, 2]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                         if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "shuffle(cand_maked_pos)\n",
    "cand_maked_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 21, 23, 17, 2, 28, 18, 15, 11, 27, 17, 20, 25, 8, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 21, 23, 17, 2, 3, 18, 15, 11, 27, 17, 20, 25, 8, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "masked_tokens, masked_pos = [], []\n",
    "import random\n",
    "rand = random.Random()\n",
    "for pos in cand_maked_pos[:n_pred]:  \n",
    "           masked_pos.append(pos)\n",
    "           masked_tokens.append(input_ids[pos])\n",
    "           ran1 = rand.uniform(0.5,0.8)\n",
    "           if   ran1< 0.8 and ran1>0.5:  # 80%\n",
    "               input_ids[pos] = word_dict['[MASK]'] # make mask?\n",
    "               print(input_ids)\n",
    "           elif rand.uniform(0,1) < 0.5:  # 10%-----> replacing one word with random word...\n",
    "               print(\"hello i am masking\")\n",
    "               index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "               input_ids[pos] = word_dict[number_dict[index]] # replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 128\n",
    "n_pad = maxlen - len(input_ids)\n",
    "input_ids.extend([0] * n_pad)\n",
    "segment_ids.extend([0] * n_pad)\n",
    "if max_pred > n_pred:\n",
    "           n_pad = max_pred - n_pred\n",
    "           masked_tokens.extend([0] * n_pad)\n",
    "           masked_pos.extend([0] * n_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_ids = segment_ids[:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange,shuffle,randint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import randrange,shuffle\n",
    "batch_size = 2\n",
    "def make_batch():\n",
    "   batch = []\n",
    "   positive = negative = 0\n",
    "   while positive != batch_size/2 or negative != batch_size/2:#getting half -ve and half positive sentence \n",
    "       tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences)) \n",
    "       \n",
    "       tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "    #    print(tokens_a,tokens_b)\n",
    "\n",
    "       input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']] #token embeeing kind\n",
    "       input_ids =  [word_dict[i] for i in input_ids if i in word_dict] \n",
    "       segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    " \n",
    "       # MASK LM\n",
    "       cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                         if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "       shuffle(cand_maked_pos)\n",
    "       cand_maked_pos\n",
    "       \n",
    "       masked_tokens, masked_pos = [], []\n",
    "       import random\n",
    "       rand = random.Random()\n",
    "       for pos in cand_maked_pos[:n_pred]:  \n",
    "           masked_pos.append(pos)\n",
    "           masked_tokens.append(input_ids[pos])\n",
    "           ran1 = rand.uniform(0.5,0.8)\n",
    "           if   ran1< 0.8 and ran1>0.5:  # 80%\n",
    "               input_ids[pos] = word_dict['[MASK]'] # make mask?\n",
    "               #print(input_ids)\n",
    "           elif rand.uniform(0,1) < 0.5:  # 10%-----> replacing one word with random word...\n",
    "               #print(\"hello i am masking\")\n",
    "               index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "               input_ids[pos] = word_dict[number_dict[index]] # replace\n",
    "        # \"\"\"\n",
    "        # from line 25 to 32 we are adding mask to our vocabs\n",
    "        # \"\"\"\n",
    "\n",
    "       # Zero Paddings\n",
    "       maxlen = 128\n",
    "       n_pad = maxlen - len(input_ids)\n",
    "       input_ids.extend([0] * n_pad)\n",
    "       segment_ids.extend([0] * n_pad)\n",
    "       if max_pred > n_pred:\n",
    "           n_pad = max_pred - n_pred\n",
    "           masked_tokens.extend([0] * n_pad)\n",
    "           masked_pos.extend([0] * n_pad)\n",
    "        \n",
    "        #i got my sentence to my neural net ready...2 sent to 1 sent with pad,cls,slp etc tokens\n",
    "        #and i get segment ids, maked tokens, maskes token positons with padding to max size\n",
    "\n",
    "        #\"\"\"\n",
    "        # if (3,2 is the pair) then it falls into negative sent as sent 2 dint come next to sent 3. but if (2,3) then its a positive sentece\n",
    "        #\"\"\"\n",
    "       if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "           batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "           positive += 1\n",
    "       elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "           batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "           negative += 1\n",
    "       return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "btch = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "         126, 127]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-128-8d3e82780997>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(torch.arange(128, dtype=torch.long).expand_as(torch.tensor(input_ids)))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.arange(128, dtype=torch.long).expand_as(torch.tensor(input_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 28, 19,  3,  5, 24,  4, 18,  2, 28,  3, 15, 11, 27, 17, 20, 25,  8,\n",
       "          5,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN WORK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "d_model = 264\n",
    "n_segments = 2\n",
    "class Embedding(nn.Module):\n",
    "   def __init__(self):\n",
    "       super(Embedding, self).__init__()\n",
    "       self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "       self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
    "       self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "       self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "   def forward(self, x, seg):\n",
    "       seq_len = x.size(1)\n",
    "       pos = torch.arange(seq_len, dtype=torch.long)\n",
    "       #print(pos.shape,x.shape,seg.shape)\n",
    "       pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
    "       embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "       return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-153-89a70f0cb947>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids,segment_ids = torch.tensor(input_ids).view(1,-1),torch.tensor(segment_ids).view(1,-1)\n",
      "<ipython-input-153-89a70f0cb947>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  enc_input = emb_nn(torch.tensor(input_ids),torch.tensor(segment_ids))\n"
     ]
    }
   ],
   "source": [
    "emb_nn = Embedding()\n",
    "input_ids,segment_ids = torch.tensor(input_ids).view(1,-1),torch.tensor(segment_ids).view(1,-1)\n",
    "enc_input = emb_nn(torch.tensor(input_ids),torch.tensor(segment_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True]) tensor([ 1, 28, 19,  3,  5, 24,  4, 18,  2, 28,  3, 15, 11, 27, 17, 20, 25,  8,\n",
      "         5,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0])\n"
     ]
    }
   ],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):#for padding purposes\n",
    "   batch_size, len_q = seq_q.size()\n",
    "   batch_size, len_k = seq_k.size()\n",
    "   # eq(zero) is PAD token\n",
    "   pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "   return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
    "print(get_attn_pad_mask(input_ids, input_ids)[0][0], input_ids[0])\n",
    "#print(tor)\n",
    "attn_mask = get_attn_pad_mask(input_ids, input_ids)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "   def __init__(self):\n",
    "       super(EncoderLayer, self).__init__()\n",
    "       self.enc_self_attn = MultiHeadAttention()\n",
    "       self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "   def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "       enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "       enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "       return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three inputs and the attention mask are operated with a dot product operation that yields two outputs: \n",
    "context vectors and attention. The context vector is then passed through a linear layer and finally that yields the output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads,d_k,d_v = 8,64,64\n",
    "class MultiHeadAttention(nn.Module):\n",
    "   def __init__(self):\n",
    "       super(MultiHeadAttention, self).__init__()\n",
    "       self.W_Q = nn.Linear(d_model, d_k * n_heads) #42--->7...6 heads---> 42\n",
    "       self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "       self.W_V = nn.Linear(d_model, d_v * n_heads) #element wise product\n",
    "\n",
    "   def forward(self, Q, K, V, attn_mask):\n",
    "        #Q, K, V,-->seq input sentence--->[batch,seq_l,d_model]\n",
    "       # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "       residual, batch_size = Q, Q.size(0)\n",
    "       # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "       q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "       k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "       v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "       #attn_mask-->batch_s*num_seq\n",
    "       attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "       # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "       context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "       context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "       output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "       return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "   def __init__(self):\n",
    "       super(ScaledDotProductAttention, self).__init__()\n",
    "   def forward(self, Q, K, V, attn_mask):\n",
    "       scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "       scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "       attn = nn.Softmax(dim=-1)(scores)\n",
    "       context = torch.matmul(attn, V)  \n",
    "       return context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = MultiHeadAttention()\n",
    "attn_output,att = attention(enc_input,enc_input,enc_input,attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 264])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(PoswiseFeedForwardNet, self).__init__() #264\n",
    "      self.fc1 = nn.Linear(d_model,d_model)\n",
    "      self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self,ip):\n",
    "      return self.norm(self.fc1(ip)+ip)\n",
    "ffn = PoswiseFeedForwardNet()\n",
    "ffn(attn_output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Study-AI'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ashishgupta2598/Study-AI.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 128, 1])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask.unsqueeze(1).repeat(1, 8, 1, 1).shape # attn_mask : [batch_size x n_heads x len_q x len_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 4, 6])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(2, 3,5,6,7)\n",
    "x = torch.transpose(x, 1, 4)\n",
    "x.shape\n",
    "x = torch.tensor([[1, 2, 3],[7,8,9]])\n",
    "print(x.shape)\n",
    "x.repeat(4,3,2,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return F.gelu(input)\n",
    "import math\n",
    "def gelu(x):\n",
    "   return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 12\n",
    "class BERT(nn.Module):\n",
    "   def __init__(self):\n",
    "       super(BERT, self).__init__()\n",
    "       self.embedding = Embedding()\n",
    "       self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "       self.fc = nn.Linear(d_model, d_model)\n",
    "       self.activ1 = nn.Tanh()\n",
    "       self.linear = nn.Linear(d_model, d_model)\n",
    "       self.activ2 = GELU()\n",
    "       self.norm = nn.LayerNorm(d_model)\n",
    "       self.classifier = nn.Linear(d_model, 2)\n",
    "       # decoder is shared with embedding layer\n",
    "       embed_weight = self.embedding.tok_embed.weight\n",
    "       n_vocab, n_dim = embed_weight.size()\n",
    "       self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "       self.decoder.weight = embed_weight\n",
    "       self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "   def forward(self, input_ids, segment_ids, masked_pos):\n",
    "       output = self.embedding(input_ids, segment_ids)\n",
    "       enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "       for layer in self.layers:\n",
    "           output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "       # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "       # it will be decided by first token(CLS)\n",
    "       h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "       logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "       masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "\n",
    "       # get masked position from final output of transformer.\n",
    "       h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "       h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
    "       logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "       return logits_lm, logits_clsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "from torch import optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 8]) torch.Size([1, 8]) tensor([0])\n"
     ]
    }
   ],
   "source": [
    "model = BERT()\n",
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "print(input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1,\n",
       "   28,\n",
       "   19,\n",
       "   13,\n",
       "   5,\n",
       "   24,\n",
       "   4,\n",
       "   3,\n",
       "   2,\n",
       "   28,\n",
       "   18,\n",
       "   15,\n",
       "   11,\n",
       "   3,\n",
       "   17,\n",
       "   20,\n",
       "   25,\n",
       "   8,\n",
       "   5,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [18, 27, 0, 0, 0, 0, 0, 0],\n",
       "  [7, 13, 0, 0, 0, 0, 0, 0],\n",
       "  True]]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 cost = 22.728003\n",
      "Epoch: 0020 cost = 21.455000\n",
      "Epoch: 0030 cost = 45.288376\n",
      "Epoch: 0040 cost = 20.069708\n",
      "Epoch: 0050 cost = 15.410398\n",
      "Epoch: 0060 cost = 21.432127\n",
      "Epoch: 0070 cost = 25.298628\n",
      "Epoch: 0080 cost = 9.350975\n",
      "Epoch: 0090 cost = 31.010324\n",
      "Epoch: 0100 cost = 21.212996\n",
      "Epoch: 0110 cost = 26.127117\n",
      "Epoch: 0120 cost = 32.617741\n",
      "Epoch: 0130 cost = 7.428960\n",
      "Epoch: 0140 cost = 25.370029\n",
      "Epoch: 0150 cost = 15.179271\n",
      "Epoch: 0160 cost = 12.967487\n",
      "Epoch: 0170 cost = 15.350006\n",
      "Epoch: 0180 cost = 8.946054\n",
      "Epoch: 0190 cost = 25.587128\n",
      "Epoch: 0200 cost = 35.790142\n",
      "Epoch: 0210 cost = 24.201748\n",
      "Epoch: 0220 cost = 35.736279\n",
      "Epoch: 0230 cost = 27.080997\n",
      "Epoch: 0240 cost = 21.515652\n",
      "Epoch: 0250 cost = 8.649105\n",
      "Epoch: 0260 cost = 13.628565\n",
      "Epoch: 0270 cost = 28.945024\n",
      "Epoch: 0280 cost = 18.574001\n",
      "Epoch: 0290 cost = 20.244667\n",
      "Epoch: 0300 cost = 26.741032\n",
      "Epoch: 0310 cost = 23.025787\n",
      "Epoch: 0320 cost = 14.095778\n",
      "Epoch: 0330 cost = 32.032555\n",
      "Epoch: 0340 cost = 24.750565\n",
      "Epoch: 0350 cost = 7.071602\n",
      "Epoch: 0360 cost = 8.960644\n",
      "Epoch: 0370 cost = 44.666401\n",
      "Epoch: 0380 cost = 12.108749\n",
      "Epoch: 0390 cost = 10.587370\n",
      "Epoch: 0400 cost = 20.515333\n",
      "Epoch: 0410 cost = 35.220284\n",
      "Epoch: 0420 cost = 21.917612\n",
      "Epoch: 0430 cost = 24.972633\n",
      "Epoch: 0440 cost = 30.836740\n",
      "Epoch: 0450 cost = 13.212766\n",
      "Epoch: 0460 cost = 25.725088\n",
      "Epoch: 0470 cost = 20.139799\n",
      "Epoch: 0480 cost = 30.416973\n",
      "Epoch: 0490 cost = 16.151299\n",
      "Epoch: 0500 cost = 30.615038\n",
      "Epoch: 0510 cost = 9.647834\n",
      "Epoch: 0520 cost = 30.938763\n",
      "Epoch: 0530 cost = 30.536991\n",
      "Epoch: 0540 cost = 24.717749\n",
      "Epoch: 0550 cost = 18.613417\n",
      "Epoch: 0560 cost = 19.271635\n",
      "Epoch: 0570 cost = 18.886156\n",
      "Epoch: 0580 cost = 17.082699\n",
      "Epoch: 0590 cost = 12.874525\n",
      "Epoch: 0600 cost = 9.548258\n",
      "Epoch: 0610 cost = 20.217991\n",
      "Epoch: 0620 cost = 20.870253\n",
      "Epoch: 0630 cost = 30.159744\n",
      "Epoch: 0640 cost = 33.107658\n",
      "Epoch: 0650 cost = 35.770290\n",
      "Epoch: 0660 cost = 16.682289\n",
      "Epoch: 0670 cost = 10.114675\n",
      "Epoch: 0680 cost = 16.365351\n",
      "Epoch: 0690 cost = 17.664604\n",
      "Epoch: 0700 cost = 20.331791\n",
      "Epoch: 0710 cost = 21.068943\n",
      "Epoch: 0720 cost = 31.529474\n",
      "Epoch: 0730 cost = 30.802347\n",
      "Epoch: 0740 cost = 11.548094\n",
      "Epoch: 0750 cost = 31.414249\n",
      "Epoch: 0760 cost = 36.410141\n",
      "Epoch: 0770 cost = 25.438868\n",
      "Epoch: 0780 cost = 36.470772\n",
      "Epoch: 0790 cost = 21.781902\n",
      "Epoch: 0800 cost = 36.081516\n",
      "Epoch: 0810 cost = 28.477272\n",
      "Epoch: 0820 cost = 9.322131\n",
      "Epoch: 0830 cost = 27.649578\n",
      "Epoch: 0840 cost = 14.273647\n",
      "Epoch: 0850 cost = 35.601349\n",
      "Epoch: 0860 cost = 17.190411\n",
      "Epoch: 0870 cost = 6.127016\n",
      "Epoch: 0880 cost = 13.500071\n",
      "Epoch: 0890 cost = 13.657759\n",
      "Epoch: 0900 cost = 26.573111\n",
      "Epoch: 0910 cost = 22.017067\n",
      "Epoch: 0920 cost = 12.999182\n",
      "Epoch: 0930 cost = 20.073359\n",
      "Epoch: 0940 cost = 19.141857\n",
      "Epoch: 0950 cost = 29.917377\n",
      "Epoch: 0960 cost = 30.247740\n",
      "Epoch: 0970 cost = 28.216661\n",
      "Epoch: 0980 cost = 15.273427\n",
      "Epoch: 0990 cost = 37.192276\n",
      "Epoch: 1000 cost = 25.012970\n",
      "Epoch: 1010 cost = 23.725664\n",
      "Epoch: 1020 cost = 21.688145\n",
      "Epoch: 1030 cost = 29.250044\n",
      "Epoch: 1040 cost = 11.240823\n",
      "Epoch: 1050 cost = 23.978067\n",
      "Epoch: 1060 cost = 10.365066\n",
      "Epoch: 1070 cost = 12.764565\n",
      "Epoch: 1080 cost = 20.886442\n",
      "Epoch: 1090 cost = 23.099110\n",
      "Epoch: 1100 cost = 20.317200\n",
      "Epoch: 1110 cost = 27.500933\n",
      "Epoch: 1120 cost = 15.278649\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1121):\n",
    "       optimizer.zero_grad()\n",
    "    #    batch = make_batch()\n",
    "    #    input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "       logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "       loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "       loss_lm = (loss_lm.float()).mean()\n",
    "       loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
    "       loss = loss_lm + loss_clsf\n",
    "       if (epoch + 1) % 10 == 0:\n",
    "           print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello-------ṭ  Hello, how are you? I am Romeo.\n",
      "Hello, Romeo My name is Juliet. Nice to meet you.\n",
      "Nice meet you too. How are you today?\n",
      "Great. My baseball team won the competition.\n",
      "Oh Congratulations, Juliet\n",
      "Thanks you Romeo\n",
      "['[CLS]', 'won', 'am', 1, 'you', '[MASK]', 'too', 1, 'meet', '[SEP]', 'i', 'my', 'too', 1, '[MASK]', 0, 'great', '[SEP]']\n",
      "masked tokens list :  [19, 24]\n",
      "predict masked tokens list :  [10, 30, 27, 27, 27, 27, 27, 27]\n",
      "isNext :  False\n",
      "predict isNext :  True\n"
     ]
    }
   ],
   "source": [
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
    "print(\"hello-------ṭ \",text)\n",
    "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
    "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_clsf else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7cfccb1c4a066d62071c49aa6393a0052d539a6d2e5142ea588cae5706c4b62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
